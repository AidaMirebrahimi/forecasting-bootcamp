{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equivalent-worship",
   "metadata": {},
   "source": [
    "# Multivariate Forecasting with Autoformer\n",
    "\n",
    "This notebook outlines the application of AutoFormer, a recently-proposed transformer-based model for time series forecasting, to a Electricity Consumption Dataset. The dataset contains the hourly electricity consumption of 321 customers from 2012 to 2014. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-hollywood",
   "metadata": {},
   "source": [
    "## Package Imports and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df61c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "annual-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append parent directory to sytem path so we can import from sibling directory \n",
    "parent_dir = os.path.abspath('..')\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277276c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_forecasting.model import AutoFormer\n",
    "from transformer_forecasting.datasets import CustomDataset\n",
    "from transformer_forecasting.utils.data import split_data, get_hour_features, scale_data\n",
    "from transformer_forecasting.utils.tools import EarlyStopping, adjust_learning_rate\n",
    "from transformer_forecasting.utils.train import train_step, val_step, test\n",
    "\n",
    "from transformer_forecasting.utils.metrics import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74216dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "CSV_PATH = \"../datasets/electricity/electricity.csv\"\n",
    "\n",
    "VAL_PERC = .1\n",
    "TEST_PERC = .1\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "SEQ_LEN = 96 #Input Sequence Length\n",
    "START_TOKEN_LEN = 48 #Start Token Length\n",
    "PRED_LEN = 96 #Prediction sequence length\n",
    "FEATURES = \"M\"\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = .0001\n",
    "LRADJ = \"type1\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65945d",
   "metadata": {},
   "source": [
    " ## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-keeping",
   "metadata": {},
   "source": [
    "### Load Raw Canadian Weather Dataset File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "354d4897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 02:00:00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>234.000</td>\n",
       "      <td>415.000</td>\n",
       "      <td>215.000</td>\n",
       "      <td>1056.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>840.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>...</td>\n",
       "      <td>676.000</td>\n",
       "      <td>372.000</td>\n",
       "      <td>80100.000</td>\n",
       "      <td>4719.000</td>\n",
       "      <td>5002.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>1558.000</td>\n",
       "      <td>182.000</td>\n",
       "      <td>2162.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 03:00:00</td>\n",
       "      <td>18.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>312.000</td>\n",
       "      <td>556.000</td>\n",
       "      <td>292.000</td>\n",
       "      <td>1363.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>1102.000</td>\n",
       "      <td>271.000</td>\n",
       "      <td>...</td>\n",
       "      <td>805.000</td>\n",
       "      <td>452.000</td>\n",
       "      <td>95200.000</td>\n",
       "      <td>4643.000</td>\n",
       "      <td>6617.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>2177.000</td>\n",
       "      <td>253.000</td>\n",
       "      <td>2835.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 04:00:00</td>\n",
       "      <td>21.000</td>\n",
       "      <td>96.000</td>\n",
       "      <td>312.000</td>\n",
       "      <td>560.000</td>\n",
       "      <td>272.000</td>\n",
       "      <td>1240.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>1025.000</td>\n",
       "      <td>270.000</td>\n",
       "      <td>...</td>\n",
       "      <td>817.000</td>\n",
       "      <td>430.000</td>\n",
       "      <td>96600.000</td>\n",
       "      <td>4285.000</td>\n",
       "      <td>6571.000</td>\n",
       "      <td>64.000</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2193.000</td>\n",
       "      <td>218.000</td>\n",
       "      <td>2764.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 05:00:00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>312.000</td>\n",
       "      <td>443.000</td>\n",
       "      <td>213.000</td>\n",
       "      <td>845.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>833.000</td>\n",
       "      <td>179.000</td>\n",
       "      <td>...</td>\n",
       "      <td>801.000</td>\n",
       "      <td>291.000</td>\n",
       "      <td>94500.000</td>\n",
       "      <td>4222.000</td>\n",
       "      <td>6365.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>39.000</td>\n",
       "      <td>1315.000</td>\n",
       "      <td>195.000</td>\n",
       "      <td>2735.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 06:00:00</td>\n",
       "      <td>22.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>312.000</td>\n",
       "      <td>346.000</td>\n",
       "      <td>190.000</td>\n",
       "      <td>647.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>733.000</td>\n",
       "      <td>186.000</td>\n",
       "      <td>...</td>\n",
       "      <td>807.000</td>\n",
       "      <td>279.000</td>\n",
       "      <td>91300.000</td>\n",
       "      <td>4116.000</td>\n",
       "      <td>6298.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>1378.000</td>\n",
       "      <td>191.000</td>\n",
       "      <td>2721.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26299</th>\n",
       "      <td>2019-07-01 21:00:00</td>\n",
       "      <td>11.000</td>\n",
       "      <td>116.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>844.000</td>\n",
       "      <td>384.000</td>\n",
       "      <td>1590.000</td>\n",
       "      <td>51.000</td>\n",
       "      <td>1412.000</td>\n",
       "      <td>407.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1897.000</td>\n",
       "      <td>1589.000</td>\n",
       "      <td>166500.000</td>\n",
       "      <td>9917.000</td>\n",
       "      <td>10412.000</td>\n",
       "      <td>324.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>1870.000</td>\n",
       "      <td>162.000</td>\n",
       "      <td>2773.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26300</th>\n",
       "      <td>2019-07-01 22:00:00</td>\n",
       "      <td>11.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>749.000</td>\n",
       "      <td>371.000</td>\n",
       "      <td>1366.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>1265.000</td>\n",
       "      <td>369.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1374.000</td>\n",
       "      <td>1336.000</td>\n",
       "      <td>158800.000</td>\n",
       "      <td>6812.000</td>\n",
       "      <td>8956.000</td>\n",
       "      <td>302.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1506.000</td>\n",
       "      <td>438.000</td>\n",
       "      <td>2755.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26301</th>\n",
       "      <td>2019-07-01 23:00:00</td>\n",
       "      <td>12.000</td>\n",
       "      <td>93.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>650.000</td>\n",
       "      <td>346.000</td>\n",
       "      <td>1282.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>1079.000</td>\n",
       "      <td>308.000</td>\n",
       "      <td>...</td>\n",
       "      <td>938.000</td>\n",
       "      <td>1311.000</td>\n",
       "      <td>154300.000</td>\n",
       "      <td>6602.000</td>\n",
       "      <td>5910.000</td>\n",
       "      <td>302.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>1864.000</td>\n",
       "      <td>621.000</td>\n",
       "      <td>2650.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26302</th>\n",
       "      <td>2019-07-02 00:00:00</td>\n",
       "      <td>10.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>646.000</td>\n",
       "      <td>349.000</td>\n",
       "      <td>1261.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>1009.000</td>\n",
       "      <td>288.000</td>\n",
       "      <td>...</td>\n",
       "      <td>833.000</td>\n",
       "      <td>1227.000</td>\n",
       "      <td>141900.000</td>\n",
       "      <td>6546.000</td>\n",
       "      <td>5502.000</td>\n",
       "      <td>259.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>2623.000</td>\n",
       "      <td>783.000</td>\n",
       "      <td>2719.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26303</th>\n",
       "      <td>2019-07-02 01:00:00</td>\n",
       "      <td>11.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>648.000</td>\n",
       "      <td>337.000</td>\n",
       "      <td>1234.000</td>\n",
       "      <td>46.000</td>\n",
       "      <td>1005.000</td>\n",
       "      <td>261.000</td>\n",
       "      <td>...</td>\n",
       "      <td>783.000</td>\n",
       "      <td>1089.000</td>\n",
       "      <td>112300.000</td>\n",
       "      <td>6188.000</td>\n",
       "      <td>4934.000</td>\n",
       "      <td>115.000</td>\n",
       "      <td>31.000</td>\n",
       "      <td>2706.000</td>\n",
       "      <td>647.000</td>\n",
       "      <td>2640.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date      0       1       2       3       4        5  \\\n",
       "0      2016-07-01 02:00:00 14.000  69.000 234.000 415.000 215.000 1056.000   \n",
       "1      2016-07-01 03:00:00 18.000  92.000 312.000 556.000 292.000 1363.000   \n",
       "2      2016-07-01 04:00:00 21.000  96.000 312.000 560.000 272.000 1240.000   \n",
       "3      2016-07-01 05:00:00 20.000  92.000 312.000 443.000 213.000  845.000   \n",
       "4      2016-07-01 06:00:00 22.000  91.000 312.000 346.000 190.000  647.000   \n",
       "...                    ...    ...     ...     ...     ...     ...      ...   \n",
       "26299  2019-07-01 21:00:00 11.000 116.000   8.000 844.000 384.000 1590.000   \n",
       "26300  2019-07-01 22:00:00 11.000 103.000   8.000 749.000 371.000 1366.000   \n",
       "26301  2019-07-01 23:00:00 12.000  93.000   8.000 650.000 346.000 1282.000   \n",
       "26302  2019-07-02 00:00:00 10.000  92.000   8.000 646.000 349.000 1261.000   \n",
       "26303  2019-07-02 01:00:00 11.000  88.000   8.000 648.000 337.000 1234.000   \n",
       "\n",
       "           6        7       8  ...      311      312        313      314  \\\n",
       "0     29.000  840.000 226.000  ...  676.000  372.000  80100.000 4719.000   \n",
       "1     29.000 1102.000 271.000  ...  805.000  452.000  95200.000 4643.000   \n",
       "2     29.000 1025.000 270.000  ...  817.000  430.000  96600.000 4285.000   \n",
       "3     24.000  833.000 179.000  ...  801.000  291.000  94500.000 4222.000   \n",
       "4     16.000  733.000 186.000  ...  807.000  279.000  91300.000 4116.000   \n",
       "...      ...      ...     ...  ...      ...      ...        ...      ...   \n",
       "26299 51.000 1412.000 407.000  ... 1897.000 1589.000 166500.000 9917.000   \n",
       "26300 47.000 1265.000 369.000  ... 1374.000 1336.000 158800.000 6812.000   \n",
       "26301 48.000 1079.000 308.000  ...  938.000 1311.000 154300.000 6602.000   \n",
       "26302 48.000 1009.000 288.000  ...  833.000 1227.000 141900.000 6546.000   \n",
       "26303 46.000 1005.000 261.000  ...  783.000 1089.000 112300.000 6188.000   \n",
       "\n",
       "            315     316    317      318     319       OT  \n",
       "0      5002.000  48.000 38.000 1558.000 182.000 2162.000  \n",
       "1      6617.000  65.000 47.000 2177.000 253.000 2835.000  \n",
       "2      6571.000  64.000 43.000 2193.000 218.000 2764.000  \n",
       "3      6365.000  65.000 39.000 1315.000 195.000 2735.000  \n",
       "4      6298.000  75.000 40.000 1378.000 191.000 2721.000  \n",
       "...         ...     ...    ...      ...     ...      ...  \n",
       "26299 10412.000 324.000 21.000 1870.000 162.000 2773.000  \n",
       "26300  8956.000 302.000 20.000 1506.000 438.000 2755.000  \n",
       "26301  5910.000 302.000 18.000 1864.000 621.000 2650.000  \n",
       "26302  5502.000 259.000 33.000 2623.000 783.000 2719.000  \n",
       "26303  4934.000 115.000 31.000 2706.000 647.000 2640.000  \n",
       "\n",
       "[26304 rows x 322 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-puzzle",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The raw data has to be processed prior to initializing the dataset. This includes first filling missing entries with the column mean. Second, the data is split sequentially into train, validation and test based on `VAL_SIZE` and `TEST_SIZE` global variables. Once the datasets have been split, the columns are standardized by subtracting the mean and scaling to unti variance.\n",
    "\n",
    "Additionally, time-based features are calculated using the date index of each dataset. These time based features are used as the basis to the temporal embeddings that are added to the value embeddings prior to being fed through the Autoformer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "painted-novel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10744/3227414236.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df.fillna(df.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# FIll nan with last valid observation \n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb8dcbfc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26304,) (26304, 321)\n"
     ]
    }
   ],
   "source": [
    "# Carve out date index and features\n",
    "dates = pd.to_datetime(df.iloc[:, 0])\n",
    "data = df.iloc[:, 1:].values\n",
    "\n",
    "print(dates.shape, data.shape)\n",
    "col_name_list = df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "driven-watershed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21044, 321) (2630, 321) (2630, 321)\n",
      "(21044, 4) (2630, 4) (2630, 321)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, test and validation\n",
    "n_samples = data.shape[0]\n",
    "n_val = int(n_samples * VAL_PERC)\n",
    "n_test = int(n_samples * TEST_PERC)\n",
    "(train_data, train_dates), (val_data, val_dates), (test_data, test_dates) = split_data(data, dates, n_val, n_test)\n",
    "\n",
    "# Get Time Based Features for each dataset\n",
    "train_time_feat = get_hour_features(train_dates)\n",
    "val_time_feat = get_hour_features(val_dates)\n",
    "test_time_feat = get_hour_features(test_dates)\n",
    "\n",
    "# Standardize features for each dataset \n",
    "train_data = scale_data(train_data) \n",
    "val_data = scale_data(val_data)\n",
    "test_data = scale_data(test_data)\n",
    "\n",
    "print(train_data.shape, val_data.shape, test_data.shape)\n",
    "print(train_time_feat.shape, val_time_feat.shape, test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "490126d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train dataset and train dataloader\n",
    "train_dataset = CustomDataset(data=train_data,\n",
    "                                   time_feat=train_time_feat,\n",
    "                                   seq_len=SEQ_LEN,\n",
    "                                   start_token_len=START_TOKEN_LEN,\n",
    "                                   pred_len=PRED_LEN\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "# Initialize val dataset and val dataloader\n",
    "val_dataset = CustomDataset(data=val_data,\n",
    "                                   time_feat=val_time_feat,\n",
    "                                   seq_len=SEQ_LEN,\n",
    "                                   start_token_len=START_TOKEN_LEN,\n",
    "                                   pred_len=PRED_LEN\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "# Initialize test dataset and test dataloader\n",
    "test_dataset = CustomDataset(data=test_data,\n",
    "                                   time_feat=test_time_feat,\n",
    "                                   seq_len=SEQ_LEN,\n",
    "                                   start_token_len=START_TOKEN_LEN,\n",
    "                                   pred_len=PRED_LEN\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-invalid",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e57a5",
   "metadata": {},
   "source": [
    "###  Autoformer Overview\n",
    "\n",
    "Autoformer is a tranformer architecture for LSTF that aims to surmount the quadratic complexity of the attention mechanism and enhance prediction accuracy. In doing so, the Autoformer proposes two novel components, the Series Deocmposition Block and the Autocorrelation Block, that function as follows:\n",
    "\n",
    "- **Series Decomposition Block:** Block that decomposes series into seasonal and trend-cyclical components. Used throughout the architecture to allow for the progressive decomposition of complex time series. \n",
    "- **Autocorrelation Block:** Self attention mechanism that conducts dependecy discovery and representation aggregation at the subseries level. Assuming that sub-series at the same phase position exhibit similar temporal processes, the Autocorrelation block construct series level connections based on the process similarity which is derived using series periodicity. \n",
    "\n",
    "At the top level, autoformer consists of an encoder and a decoder, each of which is composed sets of Auto-Correlation, Series Decomposition and Feed Forward Blocks with residual connections. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"800\" alt=\"Screen Shot 2021-09-28 at 5 41 20 PM\" src=\"https://user-images.githubusercontent.com/34798787/149202576-720856f7-c827-4e3d-85d7-0053328e1c8d.png\">  \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-coverage",
   "metadata": {},
   "source": [
    "### Initialize Model\n",
    "With a high level understanding of the architecture in place, the following sections will outline the application of Autoformer to the Canadian Weather Dataset. The first step is to initialize the Autoformer model with the appropriate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ec7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with input and output dimension and optional parameter factor \n",
    "af_model = AutoFormer(\n",
    "    seq_len=SEQ_LEN, \n",
    "    label_len=START_TOKEN_LEN,\n",
    "    pred_len=PRED_LEN,\n",
    "    enc_in=321, \n",
    "    dec_in=321,\n",
    "    c_out=321, \n",
    "    factor=3,\n",
    "    freq=\"h\"\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "af_model.to(DEVICE)\n",
    "\n",
    "# Initialize optimizer to train model\n",
    "model_optim = optim.Adam(af_model.parameters(), lr=LR)\n",
    "\n",
    "# Define Loss \n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b77f",
   "metadata": {},
   "source": [
    "## Train and Validate\n",
    "Now that we have initialized the model and the dataset, we can proceed to train and validate the model. For each epoch, the validation loss is recorded and training is early stopped if the validation loss increases on consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Step\n",
    "run_name = f\"run_{str(datetime.now().strftime('%s'))}\"\n",
    "\n",
    "# Define path to save model checkoints\n",
    "path = os.path.join(\"./checkpoints/\", run_name)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(patience=2, verbose=True)\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_step(af_model, train_loader, model_optim, criterion, DEVICE, PRED_LEN, START_TOKEN_LEN)\n",
    "    val_loss = val_step(af_model, val_loader, criterion, DEVICE, PRED_LEN, START_TOKEN_LEN)\n",
    "\n",
    "    early_stopping(val_loss, af_model, path)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early Stopping\")\n",
    "        break\n",
    "    \n",
    "    adjust_learning_rate(model_optim, epoch+1, adjust_type=\"type1\", lr=LR)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-israel",
   "metadata": {},
   "source": [
    "### Visualize Train and Validation Performance\n",
    "The Mean Squared Error of the models performance accross epochs on both the train and validation set are plotted/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Plot\n",
    "f, axarr = plt.subplots(2, 1, figsize=(20, 20)) \n",
    "\n",
    "# Get X Index list\n",
    "x_index = [i for i in range(len(train_loss_list))]\n",
    "\n",
    "# Plot and Label \n",
    "axarr[0].plot(x_index, train_loss_list)\n",
    "axarr[0].set_title(\"Train Loss\")\n",
    "axarr[0].set_xlabel(\"Epoch\")\n",
    "axarr[0].set_ylabel(\"Mean Squared Error\")\n",
    "axarr[1].plot(x_index, val_loss_list)\n",
    "axarr[1].set_title(\"Validation Loss\")\n",
    "axarr[1].set_xlabel(\"Epoch\")\n",
    "axarr[1].set_ylabel(\"Mean Squared Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-harris",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "With the trained model from the previous step, we can apply it to the test set to get an unbias estimate of the models performance. Additionally, we can visualize the results to build some intuition about the forecasts being generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, pred, true = test(af_model, test_loader, run_name, DEVICE, PRED_LEN, START_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-belief",
   "metadata": {},
   "source": [
    "### Visualize Predictions\n",
    "\n",
    "To build an intuition about the forecasting produced by the model, we sample a set of 10 variables and plot a single prediction for each. This includes plotting the input, output and ground truth and a single figure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_indices = np.random.choice(range(pred.shape[0]), 1, replace=False)\n",
    "ss_pred = pred[ss_indices]\n",
    "ss_true = true[ss_indices]\n",
    "ss_input = input[ss_indices]\n",
    "\n",
    "col_indices = list(np.random.choice(range(ss_pred.shape[2]), 10, replace=False))\n",
    "print(len(col_indices), ss_pred.shape, ss_true.shape, ss_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(len(col_indices), ss_pred.shape[0], figsize=(20, 80))\n",
    "for fig_x, i in enumerate(col_indices):\n",
    "    series_preds = ss_pred[:, :, i].squeeze()\n",
    "    series_trues = ss_true[:, :, i].squeeze()\n",
    "    series_inputs = ss_input[:, :, i].squeeze()\n",
    "    \n",
    "    input_len = series_inputs.shape[0]\n",
    "    pred_gt_len = series_preds.shape[0]\n",
    "    input_x = np.array([i for i in range(input_len)])\n",
    "    x = np.array([i for i in range(input_len, input_len+pred_gt_len)])\n",
    "    axarr[fig_x].plot(x, series_preds, c=\"blue\", label=\"predictions\")\n",
    "    axarr[fig_x].plot(x, series_trues, c=\"red\", label=\"ground truth\")\n",
    "    axarr[fig_x].plot(input_x, series_inputs, c=\"green\", label=\"input\")\n",
    "    axarr[fig_x].legend()\n",
    "    axarr[fig_x].set_title(col_name_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-korean",
   "metadata": {},
   "source": [
    "### Quantitative Results\n",
    "To assess the performance of AutoFormer on the Canadian Weather dataset, we calculate its performance on the test set using a set of metrics that are commonly used in time series forecasting. The metrics include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percental Error (MAPE) and Mean Squared Percentage Error (MSPE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, mse, rmse, mape, mspe = metric(pred, true)\n",
    "print(f\"MAE: {mae} MSE: {mse} RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-intent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoformer",
   "language": "python",
   "name": "autoformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
